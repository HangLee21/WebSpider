# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html
import os
import time
from datetime import datetime

import pandas as pd
from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter
from scrapy.downloadermiddlewares.retry import get_retry_request
from scrapy.exceptions import DropItem
from scrapy.http import HtmlResponse


class ContractspiderSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesnâ€™t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class ContractspiderDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


import time
from scrapy.downloadermiddlewares.retry import get_retry_request


class RotateProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_requests.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}

    @classmethod
    def from_crawler(cls, crawler):
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        request.meta['proxy'] = new_proxy
        spider.custom_logger.info(f"ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†é200çŠ¶æ€è¯·æ±‚ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°åˆ™è¿”å›ç©ºå“åº”ï¼Œé¿å…ç¨‹åºä¸­æ–­"""

        if response.status != 200:
            start_date = request.meta.get('searchPlacardStartDate', '')
            end_date = request.meta.get('searchPlacardEndDate', '')
            page = request.meta.get('page', '')
            url = request.url
            retry_count = self.failed_urls.get(url, 0) + 1
            self.failed_urls[url] = retry_count

            if retry_count >= self.MAX_RETRY_COUNT:
                spider.custom_logger.error(
                    f"[è·³è¿‡] Start Date {start_date}, End Date {end_date}, Page {page} - å¤±è´¥ {retry_count} æ¬¡ï¼ŒçŠ¶æ€ç  {response.status}"
                )
                self.save_failed_json(start_date, end_date, page, url, spider)

                # âœ… æ„é€ ç©ºå“åº”ï¼Œé¿å…åç»­å¤„ç†å¤±è´¥å†…å®¹
                return HtmlResponse(
                    url=response.url,
                    status=response.status,
                    request=request,
                    body=b"",
                    encoding='utf-8'
                )

            # âœ… é‡è¯•é€»è¾‘
            time.sleep(5)
            new_proxy = self.get_new_proxy()
            request.meta['proxy'] = new_proxy
            spider.custom_logger.warning(
                f"[é‡è¯•] çŠ¶æ€ç  {response.status} ç¬¬ {retry_count} æ¬¡ï¼Œä½¿ç”¨ä»£ç† {new_proxy} - {page}"
            )

            retry_request = get_retry_request(request, spider=spider, reason=f"Status {response.status}")
            if retry_request:
                retry_request.meta['proxy'] = new_proxy
                retry_request.dont_filter = True
                return retry_request
            else:
                return HtmlResponse(
                    url=response.url,
                    status=response.status,
                    request=request,
                    body=b"",
                    encoding='utf-8'
                )

        return response  # æ­£å¸¸å“åº”è¿”å›

    def save_failed_json(self, start_date, end_date, page, url, spider):
        """å°†å¤±è´¥çš„è¯·æ±‚ä¿¡æ¯ä¿å­˜åˆ° JSON æ–‡ä»¶"""
        failed_data = {
            "start_date": start_date,
            "end_date": end_date,
            "page": page,
            "url": url
        }

        # **æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨**
        if os.path.exists(self.FAILED_JSON_FILE):
            with open(self.FAILED_JSON_FILE, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    if not isinstance(data, list):  # é¿å…æ–‡ä»¶æŸåå¯¼è‡´çš„é”™è¯¯
                        data = []
                except json.JSONDecodeError:
                    data = []
        else:
            data = []

        # **è¿½åŠ æ–°çš„å¤±è´¥è¯·æ±‚**
        data.append(failed_data)

        # **å†™å› JSON æ–‡ä»¶**
        with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

        spider.custom_logger.info(f"å·²å°†å¤±è´¥è¯·æ±‚ä¿å­˜åˆ° {self.FAILED_JSON_FILE}: {failed_data}")




class DetailProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_detail.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}

    @classmethod
    def from_crawler(cls, crawler):
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        request.meta['proxy'] = new_proxy
        spider.custom_logger.info(f"ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†403æˆ–å…¶ä»–é”™è¯¯çŠ¶æ€ï¼Œè¿›è¡Œé‡è¯•æˆ–è®°å½•å¤±è´¥URL"""
        if response.status != 200:
            retry_times = request.meta.get('retry_times', 0)

            if retry_times < self.MAX_RETRY_COUNT:
                # å¢åŠ é‡è¯•æ¬¡æ•°
                retry_times += 1
                new_request = request.copy()
                new_request.meta['retry_times'] = retry_times
                new_request.dont_filter = True  # é¿å…è¢« Scrapy è¿‡æ»¤æ‰
                time.sleep(5)  # é¿å…è¯·æ±‚è¿‡å¿«
                new_proxy = self.get_new_proxy()
                new_request.meta['proxy'] = new_proxy
                spider.custom_logger.warning(f"é‡è¯• {retry_times}/{self.MAX_RETRY_COUNT} - {request.url}ï¼ŒçŠ¶æ€ç : {response.status}")
                return new_request
            else:
                # è®°å½•å¤±è´¥çš„è¯·æ±‚
                self.record_failed_request(request.url, response.status)
                spider.custom_logger.error(f"è¯·æ±‚å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {request.url} çŠ¶æ€ç : {response.status}")
                # è¿”å›ä¸€ä¸ªâ€œç©ºâ€çš„å“åº”å¯¹è±¡ï¼Œä½¿ pipeline ç»§ç»­å¾€ä¸‹èµ°
                return HtmlResponse(
                    url=request.url,
                    status=response.status,
                    body=b"",  # ç©ºå†…å®¹
                    encoding='utf-8',
                    request=request
                )

        return response

    def process_exception(self, request, exception, spider):
        """å¤„ç†è¯·æ±‚å¼‚å¸¸ï¼Œä¾‹å¦‚ä»£ç†å¤±æ•ˆ"""
        retry_times = request.meta.get('retry_times', 0)

        if retry_times < self.MAX_RETRY_COUNT:
            retry_times += 1
            new_request = request.copy()
            new_request.meta['retry_times'] = retry_times
            new_request.dont_filter = True
            new_proxy = self.get_new_proxy()
            new_request.meta['proxy'] = new_proxy
            spider.custom_logger.warning(f"è¯·æ±‚å¼‚å¸¸ {exception}ï¼Œé‡è¯• {retry_times}/{self.MAX_RETRY_COUNT} - {request.url}")
            return new_request
        else:
            self.record_failed_request(request.url, str(exception))
            spider.custom_logger.error(f"è¯·æ±‚å¼‚å¸¸å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {request.url} å¼‚å¸¸: {exception}")
            raise IgnoreRequest(f"è¯·æ±‚å¼‚å¸¸å¤±è´¥: {request.url}")

    def record_failed_request(self, url, reason):
        """è®°å½•å¤±è´¥è¯·æ±‚åˆ° JSON æ–‡ä»¶"""
        if url not in self.failed_urls:
            self.failed_urls[url] = reason

        with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(self.failed_urls, f, ensure_ascii=False, indent=4)


import json
import os
import random
import requests
from scrapy.exceptions import IgnoreRequest

class AttachmentProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_attachment.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}  # è®°å½•å¤±è´¥ URL åŠå…¶é‡è¯•æ¬¡æ•°

    @classmethod
    def from_crawler(cls, crawler):
        """ä» Scrapy é…ç½®æ–‡ä»¶ settings.py è·å–ä»£ç† API"""
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        if new_proxy:
            request.meta['proxy'] = new_proxy
            spider.custom_logger.info(f"ğŸ›¡ï¸ ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†å¼‚å¸¸å“åº”ï¼ˆ403ã€500ï¼‰ï¼Œè¿›è¡Œé‡è¯•æˆ–è®°å½•å¤±è´¥"""
        if response.status in [403, 500]:  # ä»£ç†è¢«å°æˆ–æœåŠ¡å™¨é”™è¯¯
            retry_count = request.meta.get("retry_count", 0)

            if retry_count < self.MAX_RETRY_COUNT:
                new_proxy = self.get_new_proxy()
                if new_proxy:
                    request.meta["proxy"] = new_proxy
                    request.meta["retry_count"] = retry_count + 1
                    spider.custom_logger.warning(f"âš ï¸ è¯·æ±‚ {request.url} å¤±è´¥ï¼Œä½¿ç”¨æ–°ä»£ç† {new_proxy} è¿›è¡Œç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                    return request  # é‡æ–°å°è¯•è¯·æ±‚

            # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•å¤±è´¥ URL
            self.failed_urls[request.url] = retry_count + 1
            spider.custom_logger.error(f"âŒ è¯·æ±‚ {request.url} å¤±è´¥ {self.MAX_RETRY_COUNT} æ¬¡ï¼Œè®°å½•å¤±è´¥")

            # è®°å½•å¤±è´¥çš„ URL åˆ° JSON æ–‡ä»¶
            self.save_failed_urls(spider)


            return HtmlResponse(
                url=request.url,
                status=response.status,
                body=b"",  # ç©ºå†…å®¹
                encoding='utf-8',
                request=request
            )

        return response

    def save_failed_urls(self, spider):
        """ä¿å­˜å¤±è´¥çš„ URL åˆ° JSON æ–‡ä»¶"""
        if self.failed_urls:
            with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
                json.dump(self.failed_urls, f, indent=4, ensure_ascii=False)
            spider.custom_logger.info(f"ğŸ“„ å¤±è´¥çš„ URL å·²ä¿å­˜åˆ° {self.FAILED_JSON_FILE}")

