# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html
import os
import time
from datetime import datetime

import pandas as pd
from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter
from scrapy.downloadermiddlewares.retry import get_retry_request
from scrapy.exceptions import DropItem


class ContractspiderSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesnâ€™t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class ContractspiderDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


import time
import json
import logging
from scrapy.utils.request import fingerprint  # âœ… é€‚é…æ–°ç‰ˆ Scrapy
from scrapy.downloadermiddlewares.retry import get_retry_request


class RotateProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_requests.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}

    @classmethod
    def from_crawler(cls, crawler):
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        request.meta['proxy'] = new_proxy
        logging.info(f"ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†403ï¼Œé‡è¯•æˆ–è®°å½•å¤±è´¥URL"""
        if response.status != 200:
            logging.error(f'error {response.text}')
            start_date = request.meta['searchPlacardStartDate']
            end_date = request.meta['searchPlacardEndDate']
            page = request.meta['page']
            url = request.url
            fingerprint_hash = fingerprint(request)  # âœ… è®¡ç®—å”¯ä¸€è¯·æ±‚æŒ‡çº¹
            self.failed_urls[fingerprint_hash] = self.failed_urls.get(fingerprint_hash, 0) + 1

            # **è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•å¤±è´¥URLå¹¶ç»§ç»­**
            if self.failed_urls[fingerprint_hash] >= self.MAX_RETRY_COUNT:
                logging.error(
                    f"Start Date {start_date} End Date {end_date} Page {page} 403 è¶…è¿‡ {self.MAX_RETRY_COUNT} æ¬¡ï¼Œæ”¾å¼ƒé‡è¯•ï¼")
                self.save_failed_json(start_date, end_date, page)
                return response  # **âœ… ç›´æ¥è¿”å› responseï¼Œè®© Scrapy ç»§ç»­æ‰§è¡Œ**

            # **æ›´æ¢ä»£ç†å¹¶é‡è¯•**
            time.sleep(5)  # é¿å…è¯·æ±‚è¿‡å¿«
            new_proxy = self.get_new_proxy()
            request.meta['proxy'] = new_proxy
            logging.warning(f"403 é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨æ–°ä»£ç† {new_proxy} é‡æ–°è¯·æ±‚ {page}")

            retry_request = get_retry_request(request, spider=spider, reason=f"403 error with proxy {new_proxy}")
            if retry_request:
                return retry_request  # **âœ… è¿”å›æ–° Request è¿›è¡Œé‡è¯•**
            else:
                return response  # **âœ… é¿å… NoneType é”™è¯¯ï¼Œç»§ç»­åç»­çˆ¬å–**

        return response  # **âœ… æ­£å¸¸è¯·æ±‚è¿”å› Response**

    def save_failed_json(self, start_date, end_date, page, url):
        """å°†å¤±è´¥çš„è¯·æ±‚ä¿¡æ¯ä¿å­˜åˆ° JSON æ–‡ä»¶"""
        failed_data = {
            "start_date": start_date,
            "end_date": end_date,
            "page": page,
            "url": url
        }

        # **æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨**
        if os.path.exists(self.FAILED_JSON_FILE):
            with open(self.FAILED_JSON_FILE, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    if not isinstance(data, list):  # é¿å…æ–‡ä»¶æŸåå¯¼è‡´çš„é”™è¯¯
                        data = []
                except json.JSONDecodeError:
                    data = []
        else:
            data = []

        # **è¿½åŠ æ–°çš„å¤±è´¥è¯·æ±‚**
        data.append(failed_data)

        # **å†™å› JSON æ–‡ä»¶**
        with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

        logging.info(f"å·²å°†å¤±è´¥è¯·æ±‚ä¿å­˜åˆ° {self.FAILED_JSON_FILE}: {failed_data}")


import logging
import json
from scrapy.exceptions import IgnoreRequest


class DetailProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_detail.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}

    @classmethod
    def from_crawler(cls, crawler):
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        request.meta['proxy'] = new_proxy
        logging.info(f"ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†403æˆ–å…¶ä»–é”™è¯¯çŠ¶æ€ï¼Œè¿›è¡Œé‡è¯•æˆ–è®°å½•å¤±è´¥URL"""
        if response.status != 200:
            retry_times = request.meta.get('retry_times', 0)

            if retry_times < self.MAX_RETRY_COUNT:
                # å¢åŠ é‡è¯•æ¬¡æ•°
                retry_times += 1
                new_request = request.copy()
                new_request.meta['retry_times'] = retry_times
                new_request.dont_filter = True  # é¿å…è¢« Scrapy è¿‡æ»¤æ‰
                time.sleep(5)  # é¿å…è¯·æ±‚è¿‡å¿«
                new_proxy = self.get_new_proxy()
                new_request.meta['proxy'] = new_proxy
                logging.warning(f"é‡è¯• {retry_times}/{self.MAX_RETRY_COUNT} - {request.url}ï¼ŒçŠ¶æ€ç : {response.status}")
                return new_request
            else:
                # è®°å½•å¤±è´¥çš„è¯·æ±‚
                self.record_failed_request(request.url, response.status)
                logging.error(f"è¯·æ±‚å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {request.url} çŠ¶æ€ç : {response.status}")
                raise IgnoreRequest(f"è¯·æ±‚å¤±è´¥ï¼ˆ{response.status}ï¼‰: {request.url}")

        return response

    def process_exception(self, request, exception, spider):
        """å¤„ç†è¯·æ±‚å¼‚å¸¸ï¼Œä¾‹å¦‚ä»£ç†å¤±æ•ˆ"""
        retry_times = request.meta.get('retry_times', 0)

        if retry_times < self.MAX_RETRY_COUNT:
            retry_times += 1
            new_request = request.copy()
            new_request.meta['retry_times'] = retry_times
            new_request.dont_filter = True
            new_proxy = self.get_new_proxy()
            new_request.meta['proxy'] = new_proxy
            logging.warning(f"è¯·æ±‚å¼‚å¸¸ {exception}ï¼Œé‡è¯• {retry_times}/{self.MAX_RETRY_COUNT} - {request.url}")
            return new_request
        else:
            self.record_failed_request(request.url, str(exception))
            logging.error(f"è¯·æ±‚å¼‚å¸¸å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {request.url} å¼‚å¸¸: {exception}")
            raise IgnoreRequest(f"è¯·æ±‚å¼‚å¸¸å¤±è´¥: {request.url}")

    def record_failed_request(self, url, reason):
        """è®°å½•å¤±è´¥è¯·æ±‚åˆ° JSON æ–‡ä»¶"""
        if url not in self.failed_urls:
            self.failed_urls[url] = reason

        with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(self.failed_urls, f, ensure_ascii=False, indent=4)


import json
import logging
import os
import random
import requests
from scrapy.exceptions import IgnoreRequest

class AttachmentProxyMiddleware:
    MAX_RETRY_COUNT = 5  # å…è®¸çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    FAILED_JSON_FILE = "failed_attachment.json"  # å¤±è´¥è¯·æ±‚å­˜å‚¨æ–‡ä»¶

    def __init__(self, api_url):
        self.api_url = api_url
        self.failed_urls = {}  # è®°å½•å¤±è´¥ URL åŠå…¶é‡è¯•æ¬¡æ•°

    @classmethod
    def from_crawler(cls, crawler):
        """ä» Scrapy é…ç½®æ–‡ä»¶ settings.py è·å–ä»£ç† API"""
        api_url = crawler.settings.get('PROXY_API_URL', '')
        return cls(api_url)

    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†IP"""
        return self.api_url  # å‡è®¾ API ç›´æ¥è¿”å›ä»£ç†åœ°å€

    def process_request(self, request, spider):
        """ä¸ºè¯·æ±‚è®¾ç½®ä»£ç†"""
        new_proxy = self.get_new_proxy()
        if new_proxy:
            request.meta['proxy'] = new_proxy
            logging.info(f"ğŸ›¡ï¸ ä½¿ç”¨ä»£ç† {new_proxy} è®¿é—® {request.url}")

    def process_response(self, request, response, spider):
        """å¤„ç†å¼‚å¸¸å“åº”ï¼ˆ403ã€500ï¼‰ï¼Œè¿›è¡Œé‡è¯•æˆ–è®°å½•å¤±è´¥"""
        if response.status in [403, 500]:  # ä»£ç†è¢«å°æˆ–æœåŠ¡å™¨é”™è¯¯
            retry_count = request.meta.get("retry_count", 0)

            if retry_count < self.MAX_RETRY_COUNT:
                new_proxy = self.get_new_proxy()
                if new_proxy:
                    request.meta["proxy"] = new_proxy
                    request.meta["retry_count"] = retry_count + 1
                    logging.warning(f"âš ï¸ è¯·æ±‚ {request.url} å¤±è´¥ï¼Œä½¿ç”¨æ–°ä»£ç† {new_proxy} è¿›è¡Œç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                    return request  # é‡æ–°å°è¯•è¯·æ±‚

            # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•å¤±è´¥ URL
            self.failed_urls[request.url] = retry_count + 1
            logging.error(f"âŒ è¯·æ±‚ {request.url} å¤±è´¥ {self.MAX_RETRY_COUNT} æ¬¡ï¼Œè®°å½•å¤±è´¥")

            # è®°å½•å¤±è´¥çš„ URL åˆ° JSON æ–‡ä»¶
            self.save_failed_urls()

            raise IgnoreRequest(f"è¯·æ±‚ {request.url} å¤šæ¬¡å¤±è´¥ï¼Œè·³è¿‡")

        return response

    def save_failed_urls(self):
        """ä¿å­˜å¤±è´¥çš„ URL åˆ° JSON æ–‡ä»¶"""
        if self.failed_urls:
            with open(self.FAILED_JSON_FILE, "w", encoding="utf-8") as f:
                json.dump(self.failed_urls, f, indent=4, ensure_ascii=False)
            logging.info(f"ğŸ“„ å¤±è´¥çš„ URL å·²ä¿å­˜åˆ° {self.FAILED_JSON_FILE}")

