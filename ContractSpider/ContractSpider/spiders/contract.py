import scrapy
import os
import json
import logging
from datetime import datetime
from scrapy.utils.project import get_project_settings
from ContractSpider.items import ContractItem

class ContractSpider(scrapy.Spider):
    name = "contract"
    allowed_domains = ["htgs.ccgp.gov.cn"]

    data_url = 'http://htgs.ccgp.gov.cn/GS8/contractpublish/getContractByAjax?contractSign=0'

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded",
        'Connection': 'Close',
    }

    custom_settings = {
        'DOWNLOADER_MIDDLEWARES': {
            'ContractSpider.middlewares.RotateProxyMiddleware': 300,
        },
        'ITEM_PIPELINES': {
            'ContractSpider.pipelines.ContractPipeline': 300,
        }
    }

    base_payload = {
        "code": "KL4S",
        "codeResult": "eebb0586e81a4700e5758a228af0dfb5",
        "currentPage": "0",
        "isChange": "",
        "searchAgentName": "",
        "searchContractCode": "",
        "searchContractName": "",
        "searchPlacardEndDate": "",
        "searchPlacardStartDate": "",
        "searchProjCode": "",
        "searchProjName": "",
        "searchPurchaserName": "",
        "searchSupplyName": ""
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        settings = get_project_settings()
        self.start_date = kwargs.get("CONTRACT_START_DATE", settings.get("CONTRACT_START_DATE", "2024-10-01"))
        self.end_date = kwargs.get("CONTRACT_END_DATE", settings.get("CONTRACT_END_DATE", "2024-10-31"))

        self.base_payload['searchPlacardStartDate'] = self.start_date
        self.base_payload['searchPlacardEndDate'] = self.end_date

        # é…ç½® logger
        today_str = datetime.now().strftime("%Y_%m_%d")
        log_file_path = f"logs/contract_{today_str}.log"
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

        self.custom_logger = logging.getLogger(self.name)
        if not self.custom_logger.handlers:
            self.custom_logger.setLevel(logging.INFO)
            handler = logging.FileHandler(log_file_path, encoding="utf-8")
            formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(filename)s:%(lineno)d - %(message)s')
            handler.setFormatter(formatter)
            self.custom_logger.addHandler(handler)
            self.custom_logger.propagate = False
    
    def start_requests(self):
        """çˆ¬è™«å¯åŠ¨å…¥å£ï¼Œç›´æ¥è¯·æ±‚ç¬¬ä¸€é¡µ"""
        self.custom_logger.info(f"çˆ¬è™«å¯åŠ¨ï¼Œæ—¥æœŸèŒƒå›´: {self.start_date} to {self.end_date}")
        payload = self.base_payload.copy()
        payload['currentPage'] = '1'
        yield scrapy.FormRequest(
            url=self.data_url,
            method="POST",
            headers=self.headers,
            formdata=payload,
            callback=self.parse,
            meta={'page': 1, 'payload': payload}
        )

    def parse(self, response):
        page = response.meta["page"]
        payload = response.meta["payload"]

        # 1. å¤„ç†æœ€ç»ˆå¤±è´¥çš„è¯·æ±‚ï¼ˆç”±ä¸­é—´ä»¶è¿”å›ï¼‰
        if response.status != 200 or not response.body:
            self.custom_logger.error(f"âŒ ç¬¬ {page} é¡µè¯·æ±‚æœ€ç»ˆå¤±è´¥(çŠ¶æ€ç : {response.status})ï¼Œå°†è·³è¿‡å¹¶è¯·æ±‚ä¸‹ä¸€é¡µã€‚")
            yield from self.request_next_page(page, payload)
            return

        # 2. å¤„ç†JSONè§£æé”™è¯¯
        try:
            response_json = json.loads(response.text)
        except json.JSONDecodeError:
            self.custom_logger.error(f"ğŸ“„ ç¬¬ {page} é¡µè¿”å›çš„JSONæ ¼å¼é”™è¯¯ï¼Œå°†è·³è¿‡å¹¶è¯·æ±‚ä¸‹ä¸€é¡µã€‚å“åº”: {response.text[:200]}")
            yield from self.request_next_page(page, payload)
            return
        
        rows = response_json.get("rows", [])
        self.custom_logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µï¼Œè·å–åˆ° {len(rows)} æ¡æ•°æ®ã€‚")

        # 3. å¦‚æœå½“å‰é¡µè¿”å›çš„ "rows" ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºçˆ¬å–ç»“æŸ
        if not rows:
            self.custom_logger.info(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰è¿”å›æ•°æ®(rowsä¸ºç©º)ï¼Œè®¤ä¸ºå·²åˆ°è¾¾æœ«é¡µï¼Œçˆ¬è™«æ­£å¸¸ç»“æŸã€‚")
            return

        # æ­£å¸¸æå–æ•°æ®
        for row in rows:
            item = ContractItem()
            # ... å¡«å…… item çš„ä»£ç  ...
            item["sign_date"] = row.get("signDate", "").strip()
            item["publish_date"] = row.get("publishDate", "").strip()
            item["purchaser"] = row.get("purchaserName", "").strip()
            item["supplier"] = row.get("supplyName", "").strip()
            item["agent"] = row.get("agentName", "").strip()
            item["contract_link"] = f'http://htgs.ccgp.gov.cn/GS8/contractpublish/detail/{row.get("uuid", "")}?contractSign=0'
            item["project_name"] = row.get("projName", "").strip()
            item["contract_name"] = row.get("contractName", "").strip()
            
            publish_date_str = item["publish_date"].split()[0] if item["publish_date"] else ''
            if not publish_date_str:
                self.custom_logger.warning(f"è·³è¿‡ä¸€æ¡æ•°æ®ï¼Œå› ä¸ºå‘å¸ƒæ—¥æœŸä¸ºç©º: {row}")
                continue
            
            try:
                date_obj = datetime.strptime(publish_date_str, "%Y-%m-%d")
                folder_path = os.path.join("downloads", date_obj.strftime("%Y-%m"))
                os.makedirs(folder_path, exist_ok=True)
                item["file_path"] = os.path.join(folder_path, f"{date_obj.strftime('%Y-%m-%d')}.xlsx")
                yield item
            except (ValueError, KeyError) as e:
                self.custom_logger.warning(f"è·³è¿‡ä¸€æ¡æ•°æ®ï¼Œå› æ—¥æœŸæˆ–UUIDæ ¼å¼é”™è¯¯({e}): {row}")
                continue

        # 4. æˆåŠŸå¤„ç†å®Œå½“å‰é¡µæ•°æ®åï¼Œè¯·æ±‚ä¸‹ä¸€é¡µ
        yield from self.request_next_page(page, payload)

    def request_next_page(self, current_page, payload):
        """
        å¥å£®çš„ç¿»é¡µå‡½æ•°ã€‚å®ƒåªè´Ÿè´£ç”Ÿæˆä¸‹ä¸€é¡µçš„è¯·æ±‚ã€‚
        """
        next_page = current_page + 1
        payload['currentPage'] = str(next_page)
        self.custom_logger.info(f"â¡ï¸ å‡†å¤‡è¯·æ±‚ç¬¬ {next_page} é¡µ...")
        yield scrapy.FormRequest(
            url=self.data_url,
            method="POST",
            headers=self.headers,
            formdata=payload,
            callback=self.parse,
            meta={'page': next_page, 'payload': payload}
        )

    def close(self, reason):
        self.custom_logger.info(f"çˆ¬è™«å…³é—­ï¼ŒåŸå› : {reason}")