import logging

import scrapy
import json
import os
from datetime import datetime

from tqdm import tqdm

from ContractSpider.items import ContractItem


class ContractSpider(scrapy.Spider):
    name = "contract"
    allowed_domains = ["htgs.ccgp.gov.cn"]

    # API æ¥å£
    data_url = 'http://htgs.ccgp.gov.cn/GS8/contractpublish/getContractByAjax?contractSign=0'
    count_url = 'http://htgs.ccgp.gov.cn/GS8/contractpublish/getCountByAjax?contractSign=0'

    total_pages = -1  # å…ˆåˆå§‹åŒ–

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        from scrapy.utils.project import get_project_settings
        settings = get_project_settings()
        # å…¼å®¹å‘½ä»¤è¡Œå‚æ•° `-a CONTRACT_START_DATE=2025-03-04` `-a CONTRACT_END_DATE=2025-03-10`
        self.start_date = kwargs.get("CONTRACT_START_DATE", None)
        self.end_date = kwargs.get("CONTRACT_END_DATE", None)
        self.current_page = 1  # ä»1å¼€å§‹

        # å¦‚æœå‘½ä»¤è¡Œæœªæä¾›ï¼Œåˆ™ä» settings.py è¯»å–
        if not self.start_date:
            self.start_date = settings.get("CONTRACT_START_DATE", "2025-03-01")
        if not self.end_date:
            self.end_date = settings.get("CONTRACT_END_DATE", "2025-03-10")

        self.base_payload['searchPlacardEndDate'] = self.end_date
        self.base_payload['searchPlacardStartDate'] = self.start_date

        self.download_dir = "downloads"  # æŒ‡å®šä¸‹è½½ç›®å½•

        # æ—¥å¿—é…ç½®
        today = datetime.now()
        log_filename = f"contract_{today.year}_{today.month:02d}_{today.day:02d}.log"
        log_path = os.path.join("logs", log_filename)
        os.makedirs("logs", exist_ok=True)
        logging.basicConfig(
            filename=log_path,
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            filemode="w"
        )

        self.logger.info("âœ… ContractSpider åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–è¿›åº¦æ¡ï¼ˆå…ˆè®¾ä¸º Noneï¼‰
        self.progress_bar = None


    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded",
        'Connection': 'Close',
    }

    base_payload = {
        "code": "KL4S",
        "codeResult": "eebb0586e81a4700e5758a228af0dfb5",
        "currentPage": "0",
        "isChange": "",
        "searchAgentName": "",
        "searchContractCode": "",
        "searchContractName": "",
        "searchPlacardEndDate": "",
        "searchPlacardStartDate": "",
        "searchProjCode": "",
        "searchProjName": "",
        "searchPurchaserName": "",
        "searchSupplyName": ""
    }

    def start_requests(self):
        """è·å–æ€»é¡µæ•°"""
        yield scrapy.FormRequest(
            url=self.count_url,
            method="POST",
            headers=self.headers,
            formdata=self.base_payload.copy(),
            callback=self.parse_total_pages
        )

    def parse_total_pages(self, response):
        """è§£ææ€»é¡µæ•°ï¼Œå¯åŠ¨çˆ¬å–"""
        try:
            response_json = json.loads(response.text)
            total_count = int(response_json)
            page_size = 20
            self.total_pages = (total_count // page_size) + (1 if total_count % page_size != 0 else 0)

            self.logger.info(f"æ€»åˆåŒæ•°: {total_count}, æ¯é¡µ {page_size} æ¡, é¢„è®¡é¡µæ•°: {self.total_pages}")

            # åˆå§‹åŒ– tqdm è¿›åº¦æ¡
            self.progress_bar = tqdm(total=self.total_pages, desc="åˆåŒé¡µæ•°è¿›åº¦", ncols=80)

            payload = self.base_payload.copy()
            payload["currentPage"] = "1"

            yield scrapy.FormRequest(
                url=self.data_url,
                method="POST",
                headers=self.headers,
                formdata=payload,
                callback=self.parse,
                meta={"page": 1, "payload": payload}
            )
        except Exception as e:
            self.logger.error(f"è§£ææ€»é¡µæ•°å¤±è´¥: {e}")


    def parse(self, response):
        """è§£æåˆåŒæ•°æ®å¹¶å­˜å‚¨"""
        payload = response.meta["payload"]
        self.logger.info(f"ğŸ“„ å½“å‰é¡µ: {self.current_page}")
        if self.progress_bar:
            self.progress_bar.update(1)

        if response.status != 200:
            logging.error(f"error {response.status} in page {self.current_page}")
            self.current_page = self.current_page + 1
            payload["currentPage"] = str(self.current_page)

            yield scrapy.FormRequest(
                url=self.data_url,
                method="POST",
                headers=self.headers,
                formdata=payload.copy(),
                callback=self.parse,
                meta={"page": self.current_page, "payload": payload}
            )

        else:
            try:
                response_json = json.loads(response.text)
                self.current_page = int(response.meta["page"])

                for row in response_json.get("rows", []):
                    item = ContractItem()
                    item["sign_date"] = row.get("signDate", "").strip()
                    item["publish_date"] = row.get("publishDate", "").strip()
                    item["purchaser"] = row.get("purchaserName", "").strip()
                    item["supplier"] = row.get("supplyName", "").strip()
                    item["agent"] = row.get("agentName", "").strip()
                    item[
                        "contract_link"] = f'http://htgs.ccgp.gov.cn/GS8/contractpublish/detail/{row["uuid"]}?contractSign=0'
                    item["project_name"] = row.get("projName", "").strip()
                    item["contract_name"] = row.get("contractName", "").strip()

                    # **æ–‡ä»¶è·¯å¾„é€»è¾‘**
                    publish_date = item["publish_date"]

                    try:
                        # ä»…å–æ—¥æœŸéƒ¨åˆ†ï¼Œé˜²æ­¢æ—¶é—´å¯¼è‡´è§£æå¤±è´¥
                        date_obj = datetime.strptime(publish_date.split()[0], "%Y-%m-%d")
                        # å°† self.end_date è½¬æ¢ä¸º datetime å¯¹è±¡
                        end_date_obj = datetime.strptime(self.end_date, "%Y-%m-%d")
                        # ä¸åŒ…å«ç»“æŸæ—¥æœŸå½“å¤©ï¼ˆåŒ…å«æ—¶åªæœ‰00:00æ—¶åˆ»çš„æ•°æ®ï¼‰
                        if date_obj == end_date_obj:
                            continue
                        folder_path = os.path.join(self.download_dir, date_obj.strftime("%Y-%m"))  # æŒ‰æœˆåˆ†ç±»
                        os.makedirs(folder_path, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
                        file_path = os.path.join(folder_path, f"{date_obj.strftime('%Y-%m-%d')}.xlsx")  # æŒ‰å¤©å­˜æ”¾
                    except ValueError:
                        self.logger.error(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {publish_date}")
                        continue  # è·³è¿‡é”™è¯¯æ•°æ®

                    item["file_path"] = file_path  # ä¼ é€’è·¯å¾„ç»™ pipeline
                    yield item  # äº¤ç»™ pipelines å¤„ç†

                # **çˆ¬å–ä¸‹ä¸€é¡µ**
                if self.current_page <= self.total_pages:
                    self.current_page = self.current_page + 1
                    payload["currentPage"] = str(self.current_page)

                    yield scrapy.FormRequest(
                        url=self.data_url,
                        method="POST",
                        headers=self.headers,
                        formdata=payload.copy(),
                        callback=self.parse,
                        meta={"page": self.current_page, "payload": payload}
                    )
            except Exception as e:
                self.logger.error(f"è§£ææ•°æ®å¤±è´¥: {e}")


    def closed(self, reason):
        if self.progress_bar:
            self.progress_bar.close()
        self.logger.info(f"ğŸ“¦ ContractSpider çˆ¬è™«ç»“æŸï¼ŒåŸå› ï¼š{reason}")

